{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ID=8951354, Date='12/31/12 23:15', LocationDescription='STREET', Arrest=False, Domestic=False, Beat=623, District='6', CommunityArea='69', Year=2012, Latitude=41.75628399, Longitude=-87.62164472)\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- LocationDescription: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- CommunityArea: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      "\n",
      "None\n",
      "['ID', 'Date', 'LocationDescription', 'Arrest', 'Domestic', 'Beat', 'District', 'CommunityArea', 'Year', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HousingPrices\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "base = spark.read.csv(\"file:///home/maria/Documents/SparkSQLPractice/MITAnalyticsEdge/mvtWeek1.csv\",inferSchema=True,header=True)\n",
    "print(base.head())\n",
    "print(base.printSchema())\n",
    "print(base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nID: a unique identifier for each observation\\nDate: the date the crime occurred\\nLocationDescription: the location where the crime occurred\\nArrest: whether or not an arrest was made for the crime (TRUE if an arrest was made, and FALSE if an arrest was not made)\\nDomestic: whether or not the crime was a domestic crime, meaning that it was committed against a family member (TRUE if it was domestic, and FALSE if it was not domestic)\\nBeat: the area, or \"beat\" in which the crime occurred. This is the smallest regional division defined by the Chicago police department.\\nDistrict: the police district in which the crime occured. Each district is composed of many beats, and are defined by the Chicago Police Department.\\nCommunityArea: the community area in which the crime occurred. Since the 1920s, Chicago has been divided into what are called \"community areas\", of which there are now 77. The community areas were devised in an attempt to create socially homogeneous regions.\\nYear: the year in which the crime occurred.\\nLatitude: the latitude of the location at which the crime occurred.\\nLongitude: the longitude of the location at which the crime occurred.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ID: a unique identifier for each observation\n",
    "Date: the date the crime occurred\n",
    "LocationDescription: the location where the crime occurred\n",
    "Arrest: whether or not an arrest was made for the crime (TRUE if an arrest was made, and FALSE if an arrest was not made)\n",
    "Domestic: whether or not the crime was a domestic crime, meaning that it was committed against a family member (TRUE if it was domestic, and FALSE if it was not domestic)\n",
    "Beat: the area, or \"beat\" in which the crime occurred. This is the smallest regional division defined by the Chicago police department.\n",
    "District: the police district in which the crime occured. Each district is composed of many beats, and are defined by the Chicago Police Department.\n",
    "CommunityArea: the community area in which the crime occurred. Since the 1920s, Chicago has been divided into what are called \"community areas\", of which there are now 77. The community areas were devised in an attempt to create socially homogeneous regions.\n",
    "Year: the year in which the crime occurred.\n",
    "Latitude: the latitude of the location at which the crime occurred.\n",
    "Longitude: the longitude of the location at which the crime occurred.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| row_n|\n",
      "+------+\n",
      "|191641|\n",
      "+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base.createOrReplaceTempView(\"crimes\")\n",
    "#Number of rows in data\n",
    "qur = spark.sql(\"SELECT COUNT(*) as row_n FROM crimes\")\n",
    "print(qur.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#Number of columns in data\n",
    "print(len(base.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| max_id|\n",
      "+-------+\n",
      "|9181151|\n",
      "+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Max ID\n",
    "qur_max=spark.sql(\"SELECT MAX(ID) as max_id FROM crimes\")\n",
    "print(qur_max.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|min_beat|\n",
      "+--------+\n",
      "|     111|\n",
      "+--------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Min Beat\n",
    "qur_min=spark.sql(\"SELECT MIN(Beat) as min_beat FROM crimes\")\n",
    "print(qur_min.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|nr_arrests|\n",
      "+----------+\n",
      "|     15536|\n",
      "+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Nr of crimes with arrest\n",
    "qur_arr=spark.sql(\"SELECT COUNT(*) as nr_arrests FROM crimes WHERE Arrest=TRUE\")\n",
    "print(qur_arr.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|nr_arrests|\n",
      "+----------+\n",
      "|      2308|\n",
      "+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#How many observations have a LocationDescription value of ALLEY?\n",
    "qur_all=spark.sql(\"SELECT COUNT(*) as nr_arrests FROM crimes WHERE LocationDescription='ALLEY'\")\n",
    "print(qur_all.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Date          |\n",
      "+--------------+\n",
      "|12/31/12 23:15|\n",
      "|12/31/12 22:00|\n",
      "|12/31/12 22:00|\n",
      "|12/31/12 22:00|\n",
      "|12/31/12 21:30|\n",
      "|12/31/12 20:30|\n",
      "|12/31/12 20:10|\n",
      "|12/31/12 20:00|\n",
      "|12/31/12 19:00|\n",
      "|12/31/12 18:00|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "+-------+\n",
      "|Weekday|\n",
      "+-------+\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      6|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "|      5|\n",
      "+-------+\n",
      "only showing top 100 rows\n",
      "\n",
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "+-----+\n",
      "only showing top 100 rows\n",
      "\n",
      "+-------+-----+\n",
      "|Weekday|count|\n",
      "+-------+-----+\n",
      "|      3|27319|\n",
      "|      0|27397|\n",
      "|      5|27118|\n",
      "|      6|26316|\n",
      "|      1|26791|\n",
      "|      4|29284|\n",
      "|      2|27416|\n",
      "+-------+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Month|count|\n",
      "+-----+-----+\n",
      "|    7|16801|\n",
      "|   11|16063|\n",
      "|    3|15758|\n",
      "|    8|16572|\n",
      "|    5|16035|\n",
      "|    6|16002|\n",
      "|    9|16060|\n",
      "|    1|16047|\n",
      "|   10|17086|\n",
      "|    4|15280|\n",
      "|   12|16426|\n",
      "|    2|13511|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, date_format,to_date\n",
    "from datetime import datetime\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf\n",
    "\n",
    "#qur_all=spark.sql(\"SELECT Date as date FROM crimes\")\n",
    "#qur_all.show()\n",
    "\n",
    "\n",
    "print(base.select(\"Date\").show(10, truncate = False))\n",
    "\n",
    "myfunc =  udf(lambda x: datetime.strptime(x, '%m/%d/%y %H:%M'), TimestampType())\n",
    "weekd_func=udf(lambda x: x.weekday())\n",
    "month_func=udf(lambda x: x.month)\n",
    "df = base.withColumn('Weekday', weekd_func(myfunc(col('Date'))))\n",
    "df_month=base.withColumn('Month', month_func(myfunc(col('Date'))))\n",
    "#df['Month']\n",
    "df.select(df[\"Weekday\"]).show(100)\n",
    "df_month.select(df_month[\"Month\"]).show(100)\n",
    "\n",
    "df.groupBy(\"Weekday\").count().show()\n",
    "df_month.groupBy(\"Month\").count().show()\n",
    "#https://datascience-enthusiast.com/Python/DataFramesVsRDDsVsSQLSpark-Part5.html\n",
    "#http://www.dbaglobe.com/2019/03/python-date-time-string-format.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '(' expecting <EOF>(line 1, pos 18)\\n\\n== SQL ==\\nSELECT Month COUNT(*) FROM months WHERE Arrest=TRUE GROUP BY Month\\n------------------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '(' expecting <EOF>(line 1, pos 18)\n\n== SQL ==\nSELECT Month COUNT(*) FROM months WHERE Arrest=TRUE GROUP BY Month\n------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-255c8f5569af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_month\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"months\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT Month COUNT(*) FROM months WHERE Arrest=TRUE GROUP BY Month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '(' expecting <EOF>(line 1, pos 18)\\n\\n== SQL ==\\nSELECT Month COUNT(*) FROM months WHERE Arrest=TRUE GROUP BY Month\\n------------------^^^\\n\""
     ]
    }
   ],
   "source": [
    "df_month.registerTempTable(\"months\")\n",
    "qur = spark.sql(\"SELECT Month COUNT(*) FROM months WHERE Arrest=TRUE GROUP BY Month\")\n",
    "print(qur.show())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
