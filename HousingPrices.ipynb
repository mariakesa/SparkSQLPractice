{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using data from Kaggle https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data We use the train.csv which is renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "schema=StructType([StructField('ID',IntegerType(),False),StructField('ProductID',IntegerType(),False),StructField('Cost',FloatType(),False)])\n",
    "\n",
    "base = spark.read.csv(\"file:///home/maria/Documents/SparkSQLPractice/housingprices.csv\",inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`SaleType`' given input columns: [my_data._c80, my_data._c14, my_data._c19, my_data._c42, my_data._c37, my_data._c31, my_data._c54, my_data._c30, my_data._c43, my_data._c72, my_data._c6, my_data._c32, my_data._c59, my_data._c56, my_data._c24, my_data._c13, my_data._c23, my_data._c28, my_data._c25, my_data._c76, my_data._c51, my_data._c5, my_data._c11, my_data._c15, my_data._c22, my_data._c36, my_data._c71, my_data._c75, my_data._c78, my_data._c53, my_data._c40, my_data._c74, my_data._c4, my_data._c52, my_data._c17, my_data._c65, my_data._c47, my_data._c48, my_data._c29, my_data._c3, my_data._c64, my_data._c67, my_data._c70, my_data._c62, my_data._c0, my_data._c26, my_data._c1, my_data._c50, my_data._c7, my_data._c69, my_data._c63, my_data._c38, my_data._c60, my_data._c49, my_data._c61, my_data._c58, my_data._c9, my_data._c73, my_data._c16, my_data._c46, my_data._c44, my_data._c79, my_data._c55, my_data._c34, my_data._c57, my_data._c27, my_data._c21, my_data._c41, my_data._c68, my_data._c77, my_data._c20, my_data._c39, my_data._c12, my_data._c10, my_data._c8, my_data._c66, my_data._c18, my_data._c45, my_data._c33, my_data._c35, my_data._c2]; line 1 pos 7;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Project ['SaleType]\\n      +- SubqueryAlias `my_data`\\n         +- Relation[_c0#1338,_c1#1339,_c2#1340,_c3#1341,_c4#1342,_c5#1343,_c6#1344,_c7#1345,_c8#1346,_c9#1347,_c10#1348,_c11#1349,_c12#1350,_c13#1351,_c14#1352,_c15#1353,_c16#1354,_c17#1355,_c18#1356,_c19#1357,_c20#1358,_c21#1359,_c22#1360,_c23#1361,... 57 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`SaleType`' given input columns: [my_data._c80, my_data._c14, my_data._c19, my_data._c42, my_data._c37, my_data._c31, my_data._c54, my_data._c30, my_data._c43, my_data._c72, my_data._c6, my_data._c32, my_data._c59, my_data._c56, my_data._c24, my_data._c13, my_data._c23, my_data._c28, my_data._c25, my_data._c76, my_data._c51, my_data._c5, my_data._c11, my_data._c15, my_data._c22, my_data._c36, my_data._c71, my_data._c75, my_data._c78, my_data._c53, my_data._c40, my_data._c74, my_data._c4, my_data._c52, my_data._c17, my_data._c65, my_data._c47, my_data._c48, my_data._c29, my_data._c3, my_data._c64, my_data._c67, my_data._c70, my_data._c62, my_data._c0, my_data._c26, my_data._c1, my_data._c50, my_data._c7, my_data._c69, my_data._c63, my_data._c38, my_data._c60, my_data._c49, my_data._c61, my_data._c58, my_data._c9, my_data._c73, my_data._c16, my_data._c46, my_data._c44, my_data._c79, my_data._c55, my_data._c34, my_data._c57, my_data._c27, my_data._c21, my_data._c41, my_data._c68, my_data._c77, my_data._c20, my_data._c39, my_data._c12, my_data._c10, my_data._c8, my_data._c66, my_data._c18, my_data._c45, my_data._c33, my_data._c35, my_data._c2]; line 1 pos 7;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project ['SaleType]\n      +- SubqueryAlias `my_data`\n         +- Relation[_c0#1338,_c1#1339,_c2#1340,_c3#1341,_c4#1342,_c5#1343,_c6#1344,_c7#1345,_c8#1346,_c9#1347,_c10#1348,_c11#1349,_c12#1350,_c13#1351,_c14#1352,_c15#1353,_c16#1354,_c17#1355,_c18#1356,_c19#1357,_c20#1358,_c21#1359,_c22#1360,_c23#1361,... 57 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-287eac6c3604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msqlDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT SaleType FROM my_data LIMIT 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msqlDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`SaleType`' given input columns: [my_data._c80, my_data._c14, my_data._c19, my_data._c42, my_data._c37, my_data._c31, my_data._c54, my_data._c30, my_data._c43, my_data._c72, my_data._c6, my_data._c32, my_data._c59, my_data._c56, my_data._c24, my_data._c13, my_data._c23, my_data._c28, my_data._c25, my_data._c76, my_data._c51, my_data._c5, my_data._c11, my_data._c15, my_data._c22, my_data._c36, my_data._c71, my_data._c75, my_data._c78, my_data._c53, my_data._c40, my_data._c74, my_data._c4, my_data._c52, my_data._c17, my_data._c65, my_data._c47, my_data._c48, my_data._c29, my_data._c3, my_data._c64, my_data._c67, my_data._c70, my_data._c62, my_data._c0, my_data._c26, my_data._c1, my_data._c50, my_data._c7, my_data._c69, my_data._c63, my_data._c38, my_data._c60, my_data._c49, my_data._c61, my_data._c58, my_data._c9, my_data._c73, my_data._c16, my_data._c46, my_data._c44, my_data._c79, my_data._c55, my_data._c34, my_data._c57, my_data._c27, my_data._c21, my_data._c41, my_data._c68, my_data._c77, my_data._c20, my_data._c39, my_data._c12, my_data._c10, my_data._c8, my_data._c66, my_data._c18, my_data._c45, my_data._c33, my_data._c35, my_data._c2]; line 1 pos 7;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Project ['SaleType]\\n      +- SubqueryAlias `my_data`\\n         +- Relation[_c0#1338,_c1#1339,_c2#1340,_c3#1341,_c4#1342,_c5#1343,_c6#1344,_c7#1345,_c8#1346,_c9#1347,_c10#1348,_c11#1349,_c12#1350,_c13#1351,_c14#1352,_c15#1353,_c16#1354,_c17#1355,_c18#1356,_c19#1357,_c20#1358,_c21#1359,_c22#1360,_c23#1361,... 57 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "base.createOrReplaceTempView(\"my_data\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT SaleType FROM my_data LIMIT 10\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
      "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
      "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
      "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
      "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
      "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
      "\n",
      "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
      "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
      "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
      "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
      "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
      "\n",
      "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0         2   2008        WD         Normal     208500  \n",
      "1         5   2007        WD         Normal     181500  \n",
      "2         9   2008        WD         Normal     223500  \n",
      "3         2   2006        WD        Abnorml     140000  \n",
      "4        12   2008        WD         Normal     250000  \n",
      "...     ...    ...       ...            ...        ...  \n",
      "1455      8   2007        WD         Normal     175000  \n",
      "1456      2   2010        WD         Normal     210000  \n",
      "1457      5   2010        WD         Normal     266500  \n",
      "1458      4   2010        WD         Normal     142125  \n",
      "1459      6   2008        WD         Normal     147500  \n",
      "\n",
      "[1460 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"/home/maria/Documents/SparkSQLPractice/housingprices.csv\")\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
